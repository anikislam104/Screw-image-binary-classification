{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten , Dropout, MaxPooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing image and image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_directory = 'archive/train/'\n",
    "test_image_directory = 'archive/test/'\n",
    "size = 128\n",
    "good_train_images = []\n",
    "good_train_labels = []\n",
    "bad_train_images = []\n",
    "bad_train_labels = []\n",
    "\n",
    "test_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_train_images_dir = os.listdir(train_image_directory + 'good/')\n",
    "\n",
    "for i,image_name in enumerate(good_train_images_dir):\n",
    "    image = cv2.imread(train_image_directory + 'good/' + image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "    image = cv2.resize(image, (size, size))  \n",
    "    image = np.array(image)\n",
    "    good_train_images.append(image)\n",
    "    good_train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_train_images_dir = os.listdir(train_image_directory + 'not-good/')\n",
    "\n",
    "for i,image_name in enumerate(bad_train_images_dir):\n",
    "    image = cv2.imread(train_image_directory + 'not-good/' + image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "    image = cv2.resize(image, (size, size))  \n",
    "    image = np.array(image)\n",
    "    bad_train_images.append(image)\n",
    "    bad_train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test images\n",
    "test_images_dir = os.listdir(test_image_directory)\n",
    "\n",
    "for i,image_name in enumerate(test_images_dir):\n",
    "    image = cv2.imread(test_image_directory + image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "    image = cv2.resize(image, (size, size))  \n",
    "    image = np.array(image)\n",
    "    test_images.append(image)\n",
    "\n",
    "#normalize the test images\n",
    "test_images = np.array(test_images)\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "test_images = (test_images - np.mean(test_images)) / np.std(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 128, 128, 3)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To normalize the pixel values combine the good and bad images \n",
    "\n",
    "all_train_images = good_train_images + bad_train_images\n",
    "\n",
    "# Normalize the pixel values\n",
    "all_train_images = np.array(all_train_images)\n",
    "all_train_images = all_train_images.astype('float32') / 255.0\n",
    "\n",
    "# value - mean / std\n",
    "all_train_images = (all_train_images - np.mean(all_train_images)) / np.std(all_train_images)\n",
    "\n",
    "all_train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 128, 128, 3)\n",
      "(50, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "good_train_images = np.array(good_train_images)\n",
    "bad_train_images = np.array(bad_train_images)\n",
    "\n",
    "print(good_train_images.shape)\n",
    "print(bad_train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split all_train_images into good_train_images and bad_train_images\n",
    "\n",
    "good_train_images = all_train_images[:len(good_train_images)]\n",
    "bad_train_images = all_train_images[len(good_train_images):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_labels = good_train_labels + bad_train_labels\n",
    "all_train_labels = np.array(all_train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balance the imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample the bad images\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "all_train_images, all_train_labels = ros.fit_resample(all_train_images.reshape(-1, size*size*3), all_train_labels)\n",
    "all_train_images = all_train_images.reshape(-1, size, size, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 128, 128, 3)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(all_train_images.shape)\n",
    "print(all_train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 250]\n",
      " [  1 250]]\n"
     ]
    }
   ],
   "source": [
    "# find the number of labels 1 and 0\n",
    "unique, counts = np.unique(all_train_labels, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(all_train_images, all_train_labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(size, size, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 63, 63, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 127008)            0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16257152  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,258,177\n",
      "Trainable params: 16,258,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# define the evaluation metrics F1 score\n",
    "#to do\n",
    "\n",
    "# compile the model and use both precision and recall as metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (actual_positives + K.epsilon())\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [Precision(), Recall(), f1_score]\n",
    "#use tensorboard to hyperparameter tuning\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 6s 226ms/step - loss: 3.9270 - precision_7: 0.5258 - recall_7: 0.5178 - f1_score: 0.4865 - val_loss: 1.0167 - val_precision_7: 1.0000 - val_recall_7: 0.3396 - val_f1_score: 0.4843\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 5s 213ms/step - loss: 1.0497 - precision_7: 0.5895 - recall_7: 0.6853 - f1_score: 0.6282 - val_loss: 0.9636 - val_precision_7: 0.6351 - val_recall_7: 0.8868 - val_f1_score: 0.7572\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 5s 200ms/step - loss: 0.9037 - precision_7: 0.6780 - recall_7: 0.7056 - f1_score: 0.6691 - val_loss: 0.8187 - val_precision_7: 0.7895 - val_recall_7: 0.8491 - val_f1_score: 0.8224\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 5s 197ms/step - loss: 0.7679 - precision_7: 0.7670 - recall_7: 0.8020 - f1_score: 0.7735 - val_loss: 0.6930 - val_precision_7: 0.8545 - val_recall_7: 0.8868 - val_f1_score: 0.8669\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 5s 203ms/step - loss: 0.6091 - precision_7: 0.8912 - recall_7: 0.8731 - f1_score: 0.8738 - val_loss: 0.5729 - val_precision_7: 0.9273 - val_recall_7: 0.9623 - val_f1_score: 0.9457\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 5s 209ms/step - loss: 0.5714 - precision_7: 0.8529 - recall_7: 0.8832 - f1_score: 0.8574 - val_loss: 0.5329 - val_precision_7: 1.0000 - val_recall_7: 0.8302 - val_f1_score: 0.8973\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 5s 206ms/step - loss: 0.4799 - precision_7: 0.9128 - recall_7: 0.9036 - f1_score: 0.9021 - val_loss: 0.4665 - val_precision_7: 0.8689 - val_recall_7: 1.0000 - val_f1_score: 0.9349\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 5s 214ms/step - loss: 0.4259 - precision_7: 0.9235 - recall_7: 0.9188 - f1_score: 0.9180 - val_loss: 0.4072 - val_precision_7: 1.0000 - val_recall_7: 0.8868 - val_f1_score: 0.9356\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 6s 240ms/step - loss: 0.3709 - precision_7: 0.9594 - recall_7: 0.9594 - f1_score: 0.9561 - val_loss: 0.3859 - val_precision_7: 0.9804 - val_recall_7: 0.9434 - val_f1_score: 0.9586\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 6s 226ms/step - loss: 0.3292 - precision_7: 0.9550 - recall_7: 0.9695 - f1_score: 0.9608 - val_loss: 0.3504 - val_precision_7: 1.0000 - val_recall_7: 0.8868 - val_f1_score: 0.9280\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 6s 230ms/step - loss: 0.3303 - precision_7: 0.9545 - recall_7: 0.9594 - f1_score: 0.9550 - val_loss: 0.2997 - val_precision_7: 1.0000 - val_recall_7: 0.9623 - val_f1_score: 0.9806\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.2912 - precision_7: 0.9695 - recall_7: 0.9695 - f1_score: 0.9675 - val_loss: 0.3312 - val_precision_7: 1.0000 - val_recall_7: 0.8679 - val_f1_score: 0.9185\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 6s 235ms/step - loss: 0.2790 - precision_7: 0.9746 - recall_7: 0.9746 - f1_score: 0.9717 - val_loss: 0.3039 - val_precision_7: 1.0000 - val_recall_7: 0.9057 - val_f1_score: 0.9364\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 6s 230ms/step - loss: 0.2544 - precision_7: 0.9795 - recall_7: 0.9695 - f1_score: 0.9715 - val_loss: 0.2798 - val_precision_7: 1.0000 - val_recall_7: 0.8679 - val_f1_score: 0.9152\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 6s 221ms/step - loss: 0.2467 - precision_7: 0.9604 - recall_7: 0.9848 - f1_score: 0.9746 - val_loss: 0.2801 - val_precision_7: 1.0000 - val_recall_7: 0.9057 - val_f1_score: 0.9364\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 6s 221ms/step - loss: 0.2204 - precision_7: 0.9848 - recall_7: 0.9848 - f1_score: 0.9850 - val_loss: 0.2241 - val_precision_7: 1.0000 - val_recall_7: 0.9623 - val_f1_score: 0.9712\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 5s 213ms/step - loss: 0.2580 - precision_7: 0.9650 - recall_7: 0.9797 - f1_score: 0.9722 - val_loss: 0.2886 - val_precision_7: 1.0000 - val_recall_7: 0.9245 - val_f1_score: 0.9492\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 5s 208ms/step - loss: 0.2579 - precision_7: 0.9643 - recall_7: 0.9594 - f1_score: 0.9603 - val_loss: 0.3176 - val_precision_7: 1.0000 - val_recall_7: 0.8302 - val_f1_score: 0.8818\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 5s 216ms/step - loss: 0.2482 - precision_7: 0.9688 - recall_7: 0.9442 - f1_score: 0.9520 - val_loss: 0.2046 - val_precision_7: 1.0000 - val_recall_7: 1.0000 - val_f1_score: 1.0000\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 5s 208ms/step - loss: 0.2617 - precision_7: 0.9500 - recall_7: 0.9645 - f1_score: 0.9584 - val_loss: 0.2335 - val_precision_7: 1.0000 - val_recall_7: 0.9434 - val_f1_score: 0.9722\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(train_images, train_labels, epochs=20, batch_size=16, validation_data=(validation_images, validation_labels),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict the test images\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996854\n",
      "0.010934816\n",
      "0.842386\n"
     ]
    }
   ],
   "source": [
    "print(predictions.max())\n",
    "print(predictions.min())\n",
    "print(predictions.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the predictions into 0 and 1\n",
    "predictions = np.where(predictions > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_0 is good\n",
      "test_1 is good\n",
      "test_2 is good\n",
      "test_3 is good\n",
      "test_4 is good\n",
      "test_5 is good\n",
      "test_6 is good\n",
      "test_7 is good\n",
      "test_8 is good\n",
      "test_9 is good\n",
      "test_10 is good\n",
      "test_11 is good\n",
      "test_12 is good\n",
      "test_13 is good\n",
      "test_14 is not-good\n",
      "test_15 is good\n",
      "test_16 is good\n",
      "test_17 is good\n",
      "test_18 is good\n",
      "test_19 is good\n",
      "test_20 is good\n",
      "test_21 is good\n",
      "test_22 is good\n",
      "test_23 is good\n",
      "test_24 is good\n",
      "test_25 is good\n",
      "test_26 is good\n",
      "test_27 is good\n",
      "test_28 is good\n",
      "test_29 is good\n",
      "test_30 is good\n",
      "test_31 is good\n",
      "test_32 is good\n",
      "test_33 is good\n",
      "test_34 is good\n",
      "test_35 is good\n",
      "test_36 is good\n",
      "test_37 is not-good\n",
      "test_38 is good\n",
      "test_39 is not-good\n",
      "test_40 is good\n",
      "test_41 is not-good\n",
      "test_42 is good\n",
      "test_43 is not-good\n",
      "test_44 is good\n",
      "test_45 is good\n",
      "test_46 is good\n",
      "test_47 is good\n",
      "test_48 is good\n",
      "test_49 is good\n",
      "test_50 is good\n",
      "test_51 is not-good\n",
      "test_52 is good\n",
      "test_53 is good\n",
      "test_54 is good\n",
      "test_55 is good\n",
      "test_56 is good\n",
      "test_57 is good\n",
      "test_58 is not-good\n",
      "test_59 is not-good\n",
      "test_60 is good\n",
      "test_61 is good\n",
      "test_62 is not-good\n",
      "test_63 is not-good\n",
      "test_64 is good\n",
      "test_65 is good\n",
      "test_66 is good\n",
      "test_67 is good\n",
      "test_68 is good\n",
      "test_69 is good\n",
      "test_70 is good\n",
      "test_71 is good\n",
      "test_72 is good\n",
      "test_73 is not-good\n",
      "test_74 is good\n",
      "test_75 is good\n",
      "test_76 is good\n",
      "test_77 is not-good\n",
      "test_78 is good\n",
      "test_79 is good\n",
      "test_80 is good\n",
      "test_81 is good\n",
      "test_82 is good\n",
      "test_83 is good\n",
      "test_84 is good\n",
      "test_85 is good\n",
      "test_86 is good\n",
      "test_87 is good\n",
      "test_88 is good\n",
      "test_89 is not-good\n",
      "test_90 is good\n",
      "test_91 is good\n",
      "test_92 is good\n",
      "test_93 is good\n",
      "test_94 is good\n",
      "test_95 is good\n",
      "test_96 is good\n",
      "test_97 is good\n",
      "test_98 is good\n",
      "test_99 is good\n",
      "test_100 is good\n",
      "test_101 is good\n",
      "test_102 is good\n",
      "test_103 is good\n",
      "test_104 is good\n",
      "test_105 is good\n",
      "test_106 is good\n",
      "test_107 is good\n",
      "test_108 is good\n",
      "test_109 is not-good\n",
      "test_110 is good\n",
      "test_111 is good\n",
      "test_112 is good\n",
      "test_113 is good\n",
      "test_114 is good\n",
      "test_115 is good\n",
      "test_116 is good\n",
      "test_117 is good\n",
      "test_118 is good\n",
      "test_119 is good\n",
      "test_120 is good\n",
      "test_121 is good\n",
      "test_122 is good\n",
      "test_123 is good\n",
      "test_124 is good\n",
      "test_125 is good\n",
      "test_126 is good\n",
      "test_127 is good\n",
      "test_128 is not-good\n",
      "test_129 is good\n",
      "test_130 is not-good\n",
      "test_131 is good\n",
      "test_132 is good\n",
      "test_133 is good\n",
      "test_134 is good\n",
      "test_135 is not-good\n",
      "test_136 is good\n",
      "test_137 is good\n",
      "test_138 is not-good\n",
      "test_139 is good\n",
      "test_140 is good\n",
      "test_141 is good\n",
      "test_142 is good\n",
      "test_143 is good\n",
      "test_144 is good\n",
      "test_145 is good\n",
      "test_146 is good\n",
      "test_147 is good\n",
      "test_148 is good\n",
      "test_149 is good\n",
      "test_150 is good\n",
      "test_151 is good\n",
      "test_152 is good\n",
      "test_153 is good\n",
      "test_154 is good\n",
      "test_155 is good\n",
      "test_156 is good\n",
      "test_157 is good\n",
      "test_158 is good\n",
      "test_159 is good\n",
      "test_160 is good\n",
      "test_161 is good\n",
      "test_162 is good\n",
      "test_163 is good\n",
      "test_164 is good\n",
      "test_165 is good\n",
      "test_166 is not-good\n",
      "test_167 is good\n",
      "test_168 is good\n",
      "test_169 is good\n",
      "test_170 is not-good\n",
      "test_171 is good\n",
      "test_172 is good\n",
      "test_173 is good\n",
      "test_174 is good\n",
      "test_175 is not-good\n",
      "test_176 is good\n",
      "test_177 is not-good\n",
      "test_178 is good\n",
      "test_179 is good\n"
     ]
    }
   ],
   "source": [
    "#if the prediction is 1 then the image is good else it is bad\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == 1:\n",
    "        print(\"test_{} is good\".format(i))\n",
    "    else:\n",
    "        print(\"test_{} is not-good\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  22]\n",
      " [  1 158]]\n"
     ]
    }
   ],
   "source": [
    "# count the number of good and bad images\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
